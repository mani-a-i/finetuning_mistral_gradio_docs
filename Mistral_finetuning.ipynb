{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e31acf6-c452-429f-8188-84525359a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig,pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from peft import LoraModel, LoraConfig\n",
    "from pprint import pprint\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from prompt_template import template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab227f2-d878-4c6a-a263-444c173fde4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f79f5b-7128-4953-9f28-37d4d5a96728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  GPU to use\n",
    "gpu_to_use = 0\n",
    "device = torch.device(f'cuda:{gpu_to_use}' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d033f376-c5b6-4537-a603-d22c196fed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mgpu-server         \u001b[m  Thu Mar 14 22:24:11 2024  \u001b[1m\u001b[30m495.29.05\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mQuadro RTX 5000 \u001b[m |\u001b[31m 47'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   17\u001b[m / \u001b[33m16122\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m9M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m3M\u001b[m)\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e8aafd-3cbc-4914-b393-2d0b210de577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b0df82ad964451783090cd8b78f863d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Function to load model and tokenizer\n",
    "def load_model_toks():\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\",\n",
    "                                          load_in_4bit = True)\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\",\n",
    "                                                 device_map='auto',\n",
    "                                                 load_in_4bit = True)\n",
    "\n",
    "    return model, tokenizer\n",
    "    \n",
    "model, tokenizer = load_model_toks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49924338-2b7c-43f9-bb54-b64669a8b6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/mayur2/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from hf\n",
    "pretrained_dataset = load_dataset(\"ChobPT/gradio_docs_alpaca\",split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97aee06b-f2bc-4f09-9407-0cc6490a44f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'input'],\n",
       "    num_rows: 2231\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdfebb-aa77-4860-8f7f-c203889a2f79",
   "metadata": {},
   "source": [
    "#### 1.The dataset contains three cols: instruction, output, input\n",
    "#### 2.A row in the dataset is a dict which has 3 keys, namely ['instruction', 'output', 'input'], and we have 2231 such rows\n",
    "#### 3.We convert the pretrained_dataset to dict type so that it has only 3keys namely  ['instruction', 'output', 'input'], such that \n",
    "####     instruction key has all the 2231 instructions in the form of list, and same for output and input\n",
    "#### 4. Then we create a prompt with only the instruction, and pass all the instructions into the prompt.\n",
    "#### 5. Then we create a list which 2231 dict with each dict having two keys: instructions(with prompt) and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3005b16-6b65-49d3-b6ca-3b0ae7d78c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate output given a prompt\n",
    "def gen(prompt):\n",
    "    toks = tokenizer(prompt,return_tensors = 'pt').to(device)\n",
    "    output = model.generate(\n",
    "        **toks,\n",
    "        max_new_tokens = 300\n",
    "    )\n",
    "    output = output[0][len(toks.input_ids[0]):]\n",
    "    output = tokenizer.decode(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d665ae-310a-4996-acb9-8f1391e22ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  preprocessing Method 2 (Fast)\n",
    "    \n",
    "examples = pretrained_dataset.to_dict()\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "num_examples = len(examples[\"instruction\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"instruction\"][i]\n",
    "  answer = examples[\"output\"][i]\n",
    "  text_with_prompt_template = prompt_template.format(question=question)\n",
    "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "626f108a-935e-45aa-8d37-9fff80725af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pad token to unk_token,\n",
    "#  Padding on the left\n",
    "\n",
    "def tokenize_me(input):\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text = input, \n",
    "        return_tensors=\"pt\",   \n",
    "        padding=True     \n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa9ea34e-e382-4aeb-bc6e-cf1250c6342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_max_length():\n",
    "    aindex = []\n",
    "    qindex = []\n",
    "    max_length = 0\n",
    "    for i in range(len(finetuning_dataset)):\n",
    "        aindex.append(tokenize_me(finetuning_dataset[i]['answer'])['input_ids'][0].shape[0])\n",
    "        qindex.append(tokenize_me(finetuning_dataset[i]['question'])['input_ids'][0].shape[0])    \n",
    "        text = finetuning_dataset[i]['question'] + finetuning_dataset[i]['answer']\n",
    "        input_id_len = tokenize_me(text)['input_ids'][0].shape[0]\n",
    "    \n",
    "        if input_id_len>max_length:\n",
    "            max_length = input_id_len       \n",
    "        \n",
    "    \n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    \n",
    "    # Plotting the first box plot\n",
    "    axs[0].boxplot(qindex)\n",
    "    axs[0].set_title('Qustion/instruction')\n",
    "    \n",
    "    # Plotting the second box plot\n",
    "    axs[1].boxplot(aindex)\n",
    "    axs[1].set_title('Answers')\n",
    "    \n",
    "    # Displaying the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return max_length\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8863c16d-04b5-467e-8e8d-824918119673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = get_max_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4570bdee-28ea-4a58-a027-51644ba7b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec87456-8690-4fd4-aa6d-91bc5313540c",
   "metadata": {},
   "source": [
    "#### If we dont set max_length then the length of first input will be set as max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4a3539a-2b6e-4972-93ed-7b4d71bc6b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'input'],\n",
       "    num_rows: 2231\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "355fe38b-342a-4d27-8e45-903046998ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Padding Stratergy is left padding\n",
    "\n",
    "def prompt_funct(example):    \n",
    "  \n",
    "    question = example[\"instruction\"]\n",
    "    answer = example[\"output\"]\n",
    "    # print(question)\n",
    "\n",
    "    question_template = template.format(question = question)\n",
    "    input = question_template + answer    \n",
    "    return input\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a171e-3a9b-4362-bea0-c7a4f7d9ba64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8328b43e-76b8-44b3-b858-d7eb564bd1a7",
   "metadata": {},
   "source": [
    "### Training Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f567a65-cb20-4daa-967b-cc7d362703ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a6db6b1-214f-4e36-96a4-acd85282cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91baa0f8-03a8-414a-a41c-2ba735147233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbaba2-94e6-429c-a2db-ce381f184b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0b33bc0-e843-4d06-b0b5-3b186c0f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "  output_dir = \"mistral_instruct_generation\",\n",
    "  #num_train_epochs=5,\n",
    "  max_steps = 100, # comment out this line if you want to train in epochs\n",
    "  per_device_train_batch_size = 1,\n",
    "  warmup_steps = 0.03,\n",
    "  logging_steps=10,\n",
    "  save_strategy=\"epoch\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  #evaluation_strategy=\"epoch\",\n",
    "  # evaluation_strategy=\"steps\",\n",
    "  # eval_steps=20, # comment out this line if you want to evaluate at the end of each epoch\n",
    "  learning_rate=2e-4,\n",
    "  # bf16=True,\n",
    "  lr_scheduler_type='constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fbc6940-3afd-47bc-bf27-612e06ec1036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/mayur2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/user/miniconda3/envs/mayur2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = max_length\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=max_length,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=prompt_funct, # this will aplly the create_prompt mapping to all training and test dataset\n",
    "  args=args,\n",
    "  train_dataset=pretrained_dataset\n",
    "  # eval_dataset=instruct_tune_dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58935326-5b29-4675-b8d0-1145cf17a711",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020d9d4d-c0fe-4359-8f1e-db3e34cb7287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayur2",
   "language": "python",
   "name": "mayur2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
