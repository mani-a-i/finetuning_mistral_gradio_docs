{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e31acf6-c452-429f-8188-84525359a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import necessary libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,BitsAndBytesConfig,pipeline\n",
    "import os\n",
    "import pandas as pd\n",
    "import jsonlines\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from peft import LoraModel, LoraConfig\n",
    "from pprint import pprint\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "from prompt_template import template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab227f2-d878-4c6a-a263-444c173fde4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f79f5b-7128-4953-9f28-37d4d5a96728",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  GPU to use\n",
    "gpu_to_use = 0\n",
    "device = torch.device(f'cuda:{gpu_to_use}' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d033f376-c5b6-4537-a603-d22c196fed3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[37mgpu-server         \u001b[m  Sun Mar 17 14:30:31 2024  \u001b[1m\u001b[30m495.29.05\u001b[m\n",
      "\u001b[36m[0]\u001b[m \u001b[34mQuadro RTX 5000 \u001b[m |\u001b[31m 28'C\u001b[m, \u001b[32m  0 %\u001b[m | \u001b[36m\u001b[1m\u001b[33m   17\u001b[m / \u001b[33m16122\u001b[m MB | \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m9M\u001b[m) \u001b[1m\u001b[30mgdm\u001b[m(\u001b[33m3M\u001b[m)\n"
     ]
    }
   ],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26e8aafd-3cbc-4914-b393-2d0b210de577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d033d32dedb439494a0cd7c7b63590b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#  Function to load model and tokenizer\n",
    "def load_model_toks():\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\",\n",
    "                                          load_in_4bit = True)\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\",\n",
    "                                                 device_map='auto',\n",
    "                                                 load_in_4bit = True)\n",
    "\n",
    "    return model, tokenizer\n",
    "    \n",
    "model, tokenizer = load_model_toks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49924338-2b7c-43f9-bb54-b64669a8b6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/mayur2/lib/python3.10/site-packages/huggingface_hub/repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from hf\n",
    "pretrained_dataset = load_dataset(\"ChobPT/gradio_docs_alpaca\",split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97aee06b-f2bc-4f09-9407-0cc6490a44f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'input'],\n",
       "    num_rows: 2231\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fdfebb-aa77-4860-8f7f-c203889a2f79",
   "metadata": {},
   "source": [
    "#### 1.The dataset contains three cols: instruction, output, input\n",
    "#### 2.A row in the dataset is a dict which has 3 keys, namely ['instruction', 'output', 'input'], and we have 2231 such rows\n",
    "#### 3.We convert the pretrained_dataset to dict type so that it has only 3keys namely  ['instruction', 'output', 'input'], such that \n",
    "####     instruction key has all the 2231 instructions in the form of list, and same for output and input\n",
    "#### 4. Then we create a prompt with only the instruction, and pass all the instructions into the prompt.\n",
    "#### 5. Then we create a list which 2231 dict with each dict having two keys: instructions(with prompt) and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3005b16-6b65-49d3-b6ca-3b0ae7d78c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate output given a prompt\n",
    "def gen(prompt):\n",
    "    toks = tokenizer(prompt,return_tensors = 'pt').to(device)\n",
    "    output = model.generate(\n",
    "        **toks,\n",
    "        max_new_tokens = 300\n",
    "    )\n",
    "    output = output[0][len(toks.input_ids[0]):]\n",
    "    output = tokenizer.decode(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d665ae-310a-4996-acb9-8f1391e22ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  preprocessing Method 2 (Fast)\n",
    "    \n",
    "examples = pretrained_dataset.to_dict()\n",
    "prompt_template = \"\"\"### Question:\n",
    "{question}\n",
    "\n",
    "### Answer:\"\"\"\n",
    "\n",
    "num_examples = len(examples[\"instruction\"])\n",
    "finetuning_dataset = []\n",
    "for i in range(num_examples):\n",
    "  question = examples[\"instruction\"][i]\n",
    "  answer = examples[\"output\"][i]\n",
    "  text_with_prompt_template = prompt_template.format(question=question)\n",
    "  finetuning_dataset.append({\"question\": text_with_prompt_template, \"answer\": answer})\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "626f108a-935e-45aa-8d37-9fff80725af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pad token to unk_token,\n",
    "#  Padding on the left\n",
    "\n",
    "def tokenize_me(input):\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text = input, \n",
    "        return_tensors=\"pt\",   \n",
    "        padding=True     \n",
    "    )\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa9ea34e-e382-4aeb-bc6e-cf1250c6342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_max_length():\n",
    "    aindex = []\n",
    "    qindex = []\n",
    "    max_length = 0\n",
    "    for i in range(len(finetuning_dataset)):\n",
    "        aindex.append(tokenize_me(finetuning_dataset[i]['answer'])['input_ids'][0].shape[0])\n",
    "        qindex.append(tokenize_me(finetuning_dataset[i]['question'])['input_ids'][0].shape[0])    \n",
    "        text = finetuning_dataset[i]['question'] + finetuning_dataset[i]['answer']\n",
    "        input_id_len = tokenize_me(text)['input_ids'][0].shape[0]\n",
    "    \n",
    "        if input_id_len>max_length:\n",
    "            max_length = input_id_len       \n",
    "        \n",
    "    \n",
    "    fig, axs = plt.subplots(1,2)\n",
    "    \n",
    "    # Plotting the first box plot\n",
    "    axs[0].boxplot(qindex)\n",
    "    axs[0].set_title('Qustion/instruction')\n",
    "    \n",
    "    # Plotting the second box plot\n",
    "    axs[1].boxplot(aindex)\n",
    "    axs[1].set_title('Answers')\n",
    "    \n",
    "    # Displaying the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return max_length\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8863c16d-04b5-467e-8e8d-824918119673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = get_max_length()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4570bdee-28ea-4a58-a027-51644ba7b531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_length = 4096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec87456-8690-4fd4-aa6d-91bc5313540c",
   "metadata": {},
   "source": [
    "#### If we dont set max_length then the length of first input will be set as max length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4a3539a-2b6e-4972-93ed-7b4d71bc6b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'output', 'input'],\n",
       "    num_rows: 2231\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "355fe38b-342a-4d27-8e45-903046998ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Padding Stratergy is left padding\n",
    "\n",
    "def prompt_funct(example):    \n",
    "  \n",
    "    question = example[\"instruction\"]\n",
    "    answer = example[\"output\"]\n",
    "    # print(question)\n",
    "\n",
    "    question_template = template.format(question = question)\n",
    "    input = question_template + answer    \n",
    "    return input\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a171e-3a9b-4362-bea0-c7a4f7d9ba64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8328b43e-76b8-44b3-b858-d7eb564bd1a7",
   "metadata": {},
   "source": [
    "### Training Starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f567a65-cb20-4daa-967b-cc7d362703ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a6db6b1-214f-4e36-96a4-acd85282cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91baa0f8-03a8-414a-a41c-2ba735147233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): MistralForCausalLM(\n",
       "      (model): MistralModel(\n",
       "        (embed_tokens): Embedding(32000, 4096)\n",
       "        (layers): ModuleList(\n",
       "          (0-31): 32 x MistralDecoderLayer(\n",
       "            (self_attn): MistralSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "              (rotary_emb): MistralRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): MistralMLP(\n",
       "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): MistralRMSNorm()\n",
       "            (post_attention_layernorm): MistralRMSNorm()\n",
       "          )\n",
       "        )\n",
       "        (norm): MistralRMSNorm()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fbaba2-94e6-429c-a2db-ce381f184b9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f0b33bc0-e843-4d06-b0b5-3b186c0f9586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "  output_dir = \"mistral_instruct_generation\",\n",
    "  #num_train_epochs=5,\n",
    "  max_steps = 100, # comment out this line if you want to train in epochs\n",
    "  per_device_train_batch_size = 1,\n",
    "  warmup_steps = 0.03,\n",
    "  logging_steps=10,\n",
    "  save_strategy=\"epoch\",\n",
    "  gradient_accumulation_steps = 4,\n",
    "  #evaluation_strategy=\"epoch\",\n",
    "  # evaluation_strategy=\"steps\",\n",
    "  # eval_steps=20, # comment out this line if you want to evaluate at the end of each epoch\n",
    "  learning_rate=2e-4,\n",
    "  # bf16=True,\n",
    "  lr_scheduler_type='constant',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fbc6940-3afd-47bc-bf27-612e06ec1036",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/mayur2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:294: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/home/user/miniconda3/envs/mayur2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed `packing=True` to the SFTTrainer, and you are training your model with `max_steps` strategy. The dataset will be iterated until the `max_steps` are reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "\n",
    "max_seq_length = max_length\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "  model=model,\n",
    "  peft_config=peft_config,\n",
    "  max_seq_length=max_length,\n",
    "  tokenizer=tokenizer,\n",
    "  packing=True,\n",
    "  formatting_func=prompt_funct, # this will aplly the create_prompt mapping to all training and test dataset\n",
    "  args=args,\n",
    "  train_dataset=pretrained_dataset\n",
    "  # eval_dataset=instruct_tune_dataset[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58935326-5b29-4675-b8d0-1145cf17a711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/user/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 42/100 1:54:13 < 2:45:37, 0.01 it/s, Epoch 0.22/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.121800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.098100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.987900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.943500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:331\u001b[0m, in \u001b[0;36mSFTTrainer.train\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trl_activate_neftune(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m--> 331\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;66;03m# After training we make sure to retrieve back the original forward pass method\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;66;03m# for the embedding layer by removing the forward post hook.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneftune_noise_alpha \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trainer_supports_neftune:\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/transformers/trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/transformers/trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1967\u001b[0m ):\n\u001b[1;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/transformers/trainer.py:2902\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2901\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[0;32m-> 2902\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2905\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/transformers/trainer.py:2925\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2924\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2925\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2926\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2927\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/peft/peft_model.py:1091\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPOLY:\n\u001b[1;32m   1090\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task_ids\n\u001b[0;32m-> 1091\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1092\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1102\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m _get_batch_size(input_ids, inputs_embeds)\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1104\u001b[0m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:160\u001b[0m, in \u001b[0;36mBaseTuner.forward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any):\n\u001b[0;32m--> 160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1157\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1154\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1156\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1157\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1170\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1032\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     all_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (hidden_states,)\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[0;32m-> 1032\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gradient_checkpointing_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecoder_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1035\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1036\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1043\u001b[0m         hidden_states,\n\u001b[1;32m   1044\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1048\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1049\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/_compile.py:24\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dynamo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:489\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m     dynamo_config_ctx\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/_dynamo/external_utils.py:17\u001b[0m, in \u001b[0;36mwrap_inline.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/utils/checkpoint.py:482\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m context_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m noop_context_fn \u001b[38;5;129;01mor\u001b[39;00m debug \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    479\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing `context_fn` or `debug` is only supported when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    480\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_reentrant=False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    481\u001b[0m         )\n\u001b[0;32m--> 482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    484\u001b[0m     gen \u001b[38;5;241m=\u001b[39m _checkpoint_without_reentrant_generator(\n\u001b[1;32m    485\u001b[0m         function, preserve, context_fn, determinism_check, debug, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    486\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/utils/checkpoint.py:261\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    258\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mtensor_inputs)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 261\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:757\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    756\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:655\u001b[0m, in \u001b[0;36mMistralSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    653\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_proj(hidden_states)\n\u001b[1;32m    654\u001b[0m key_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_proj(hidden_states)\n\u001b[0;32m--> 655\u001b[0m value_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m query_states \u001b[38;5;241m=\u001b[39m query_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    658\u001b[0m key_states \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:355\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     expected_dtype \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    353\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(lora_A\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 355\u001b[0m output \u001b[38;5;241m=\u001b[39m lora_B(\u001b[43mlora_A\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m requires_conversion:\n\u001b[1;32m    357\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mayur2/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a5ed40-5339-40be-935e-cbca017e4c91",
   "metadata": {},
   "source": [
    "A crucial point to note is the need to add padding to the left. This approach is adopted because the model generates tokens autoregressively, meaning it continues from the last token. Adding padding to the right would cause the model to generate new tokens from these padding tokens, resulting in the output sequence including padding tokens in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4a1bf3-01be-40bd-9387-87e56e9eb314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mayur2",
   "language": "python",
   "name": "mayur2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
